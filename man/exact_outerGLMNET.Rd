% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/exact_outerGLMNET.R
\name{exact_outerGLMNET}
\alias{exact_outerGLMNET}
\title{exact_outerGLMNET}
\usage{
exact_outerGLMNET(
  mxObject,
  objective,
  adaptiveLassoWeights,
  sampleSize,
  gradientModel,
  gradientModelcpp = NULL,
  parameterNames,
  initialParameters,
  initialGradients,
  initialHessian,
  lambda,
  regIndicators,
  stepSize = 1,
  lineSearch = "none",
  c1 = 1e-04,
  c2 = 0.9,
  sig = 0.2,
  gam = 0,
  differenceApprox = "central",
  maxIter_out,
  maxIter_in,
  maxIter_line = 100,
  eps_out,
  eps_in,
  eps_WW,
  eps_numericDerivative = (1.1 * 10^(-16))^(1/3),
  scaleLambdaWithN,
  verbose = 0
)
}
\arguments{
\item{mxObject}{Object of type MxModel}

\item{objective}{which objective should be used? Possible are "ML" (Maximum Likelihood) or "Kalman" (Kalman Filter)}

\item{adaptiveLassoWeights}{weights for the adaptive lasso.}

\item{sampleSize}{sample size}

\item{gradientModel}{Object of Type MxModel which specifies how the gradients of the likelihood-function are computed (the jacobian)}

\item{gradientModelcpp}{cpptsem object which specifies how the gradients of the likelihood-function are computed (the jacobian)}

\item{parameterNames}{Vector with names of theta-parameters}

\item{initialParameters}{initial parameter estimates}

\item{initialGradients}{initial gradients of the likelihood function}

\item{initialHessian}{initial Hessian of the likelihood function}

\item{lambda}{Penalty value}

\item{regIndicators}{Names of regularized parameters}

\item{stepSize}{initial Stepsize of the outer iteration (theta_{k+1} = theta_k + stepSize \* Stepdirection) in case of lineSearch}

\item{lineSearch}{String indicating if Wolfe conditions (lineSearch = "Wolfe") should be used in the outer iteration. Set lineSearch = "dynamic" to only use line search if optimization without line search fails.}

\item{c1}{c1 constant for lineSearch. This constant controls the Armijo condition in lineSearch if lineSearch = "Wolfe"}

\item{c2}{c2 constant for lineSearch. This constant controls the Curvature condition in lineSearch if lineSearch = "Wolfe"}

\item{sig}{GLMNET & GIST: GLMNET: only relevant when lineSearch = 'GLMNET' | GIST: sigma value in Gong et al. (2013). Sigma controls the inner stopping criterion and must be in (0,1). Generally, a larger sigma enforce a steeper decrease in the regularized likelihood while a smaller sigma will result in faster acceptance of the inner iteration.}

\item{gam}{GLMNET when lineSearch = 'GLMNET'. Controls the gamma parameter in Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). An improved GLMNET for l1-regularized logistic regression. The Journal of Machine Learning Research, 13, 1999â€“2030. https://doi.org/10.1145/2020408.2020421. Defaults to 0.}

\item{differenceApprox}{Which approximation should be used for calculating the gradients in the gradientModel. central is recommended}

\item{maxIter_out}{Maximal number of outer iterations}

\item{maxIter_in}{Maximal number of inner iterations}

\item{maxIter_line}{Maximal number of iterations for the lineSearch procedure}

\item{eps_out}{Stopping criterion for outer iterations}

\item{eps_in}{Stopping criterion for inner iterations}

\item{eps_WW}{Stopping criterion for weak Wolfe line search. If the upper - lower bound of the interval is < epsWW, line search will be stopped and stepSize will be returned}

\item{eps_numericDerivative}{controls the precision of the central gradient approximation. The default (1.1 * 10^(-16))^(1/3) is derived in Nocedal, J., & Wright, S. J. (2006). Numerical optimization (2nd ed), p. 197}

\item{scaleLambdaWithN}{Boolean: Should the penalty value be scaled with the sample size? True is recommended, as the likelihood is also sample size dependent}

\item{verbose}{0 (default), 1 for convergence plot, 2 for parameter convergence plot and line search progress}
}
\description{
Performs the outer iterations of GLMNET
}
