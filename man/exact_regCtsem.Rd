% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/regCtsem.R
\name{exact_regCtsem}
\alias{exact_regCtsem}
\title{exact_regCtsem}
\usage{
exact_regCtsem(
  cpptsemObject = NULL,
  dataset = NULL,
  regIndicators,
  targetVector,
  lambdas = "auto",
  lambdasAutoLength = 50,
  penalty = "lasso",
  adaptiveLassoWeights = NULL,
  standardizeDrift = FALSE,
  returnFitIndices = TRUE,
  BICWithNAndT = TRUE,
  Tpoints = NULL,
  cvSampleCpptsemObject = NULL,
  optimizer = "GIST",
  objective,
  sparseParameters = NULL,
  stepSize = 1,
  lineSearch = "GLMNET",
  c1 = 1e-04,
  c2 = 0.9,
  sig = 10^(-5),
  gam = 0,
  initialHessianApproximation = NULL,
  maxIter_out = 100,
  maxIter_in = 1000,
  maxIter_line = 500,
  eps_out = 1e-10,
  eps_in = 1e-10,
  eps_WW = 1e-04,
  eta = 2,
  stepsizeMin = 1/(10^30),
  stepsizeMax = 10^30,
  GISTLinesearchCriterion = "monotone",
  GISTNonMonotoneNBack = 5,
  break_outer = c(parameterChange = 10^(-5)),
  scaleLambdaWithN = TRUE,
  approxFirst = FALSE,
  numStart = 0,
  controlOptimx,
  verbose = 0
)
}
\arguments{
\item{cpptsemObject}{Object of type cpptsem}

\item{dataset}{Please provide a data set in wide format compatible to ctsemOMX}

\item{regIndicators}{Labels for the regularized parameters (e.g. drift_eta1_eta2)}

\item{targetVector}{named vector with values towards which the parameters are regularized}

\item{lambdas}{vector of penalty values (tuning parameter). E.g., seq(0,1,.01). Alternatively, lambdas can be set to "auto". regCtsem will then compute an upper limit for lambda and test lambdasAutoLength increasing lambda values}

\item{lambdasAutoLength}{if lambdas == "auto", lambdasAutoLength will determine the number of lambdas tested.}

\item{penalty}{Currently supported are ridge, lasso, and adaptiveLasso}

\item{adaptiveLassoWeights}{weights for the adaptive lasso. If auto, defaults to the inverse of unregularized parameter estimates.}

\item{standardizeDrift}{Boolean: Should Drift parameters be standardized automatically using the T0VAR?}

\item{returnFitIndices}{Boolean: should fit indices be returned?}

\item{BICWithNAndT}{Boolean: TRUE = Use N and T in the formula for the BIC (-2log L + log(N+T)*k, where k is the number of parameters in the model). FALSE = Use both N in the formula for the BIC (-2log L + log(N))}

\item{Tpoints}{Number of time points (used for BICWithNAndT)}

\item{cvSampleCpptsemObject}{cppstem for cross-validation}

\item{optimizer}{Either "GIST" or "GLMNET"}

\item{objective}{which objective should be used? Possible are "ML" (Maximum Likelihood) or "Kalman" (Kalman Filter)}

\item{sparseParameters}{labeled vector with parameter estimates of the most sparse model. Required for approxFirst = 3. If regValues = "auto" the sparse parameters will be computed automatically.}

\item{stepSize}{GLMNET & GIST: initial step size of the outer iteration}

\item{lineSearch}{GLMNET: String indicating which linesearch should be used. Defaults to the one described in Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). An improved GLMNET for l1-regularized logistic regression. The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421. Alternatively (not recommended) Wolfe conditions (lineSearch = "Wolfe") can be used in the outer iteration. Setting to "none" is also not recommended!}

\item{c1}{GLMNET: c1 constant for lineSearch. This constant controls the Armijo condition in lineSearch if lineSearch = "Wolfe"}

\item{c2}{GLMNET: c2 constant for lineSearch. This constant controls the Curvature condition in lineSearch if lineSearch = "Wolfe"}

\item{sig}{GLMNET & GIST: GLMNET: only relevant when lineSearch = 'GLMNET' | GIST: sigma value in Gong et al. (2013). Sigma controls the inner stopping criterion and must be in (0,1). Generally, a larger sigma enforce a steeper decrease in the regularized likelihood while a smaller sigma will result in faster acceptance of the inner iteration.}

\item{gam}{GLMNET when lineSearch = 'GLMNET'. Controls the gamma parameter in Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). An improved GLMNET for l1-regularized logistic regression. The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421. Defaults to 0.}

\item{initialHessianApproximation}{GLMNET: Which initial hessian approximation should be used? Possible are: 'ident' for an identity matrix and 'OpenMx' (here the hessian approxmiation from the mxObject is used). If the Hessian from 'OpenMx' is not positive definite, the negative Eigenvalues will be 'flipped' to positive Eigenvalues. This works sometimes, but not always. Alternatively, a matrix can be provided which will be used as initial Hessian}

\item{maxIter_out}{GLMNET & GIST: Maximal number of outer iterations}

\item{maxIter_in}{GLMNET & GIST: Maximal number of inner iterations}

\item{maxIter_line}{GLMNET: Maximal number of iterations for the lineSearch procedure}

\item{eps_out}{GLMNET: Stopping criterion for outer iterations}

\item{eps_in}{GLMNET: Stopping criterion for inner iterations}

\item{eps_WW}{GLMNET: Stopping criterion for weak Wolfe line search. If the upper - lower bound of the interval is < epsWW, line search will be stopped and stepSize will be returned}

\item{eta}{GIST: if the current step size fails, eta will decrease the step size. Must be > 1}

\item{stepsizeMin}{GIST: Minimal acceptable step size. Must be > 0. A larger number corresponds to a smaller step from one to the next iteration. All step sizes will be computed as described by Gong et al. (2013)}

\item{stepsizeMax}{GIST: Maximal acceptable step size. Must be > stepsizeMin. A larger number corresponds to a smaller step from one to the next iteration. All step sizes will be computed as described by Gong et al. (2013)}

\item{GISTLinesearchCriterion}{criterion for accepting a step. Possible are 'monotone' which enforces a monotone decrease in the objective function or 'non-monotone' which also accepts some increase.}

\item{GISTNonMonotoneNBack}{in case of non-monotone line search: Number of preceding regM2LL values to consider}

\item{break_outer}{criterion for breaking outer iterations of GIST. See ?controlGIST for more information}

\item{scaleLambdaWithN}{Boolean: Should the penalty value be scaled with the sample size? True is recommended as the likelihood is also sample size dependent}

\item{approxFirst}{Should approximate optimization be used first to obtain start values for exact optimization?}

\item{numStart}{Used if approxFirst = 3. regCtsem will try numStart+2 starting values (+2 because it will always try the current best and the parameters provided in sparseParameters)}

\item{controlOptimx}{settings passed to optimx}

\item{verbose}{0 (default), 1 for convergence plot, 2 for parameter convergence plot and line search progress}
}
\description{
creates a regCtsem object for exact optimization
}
\details{
NOTE: Function located in file regCtsem.R
}
\author{
Jannik Orzek
}
