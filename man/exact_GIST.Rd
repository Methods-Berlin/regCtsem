% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/GIST.R
\name{exact_GIST}
\alias{exact_GIST}
\title{exact_GIST}
\usage{
exact_GIST(
  cpptsemObject,
  dataset,
  objective,
  regIndicators,
  targetVector,
  lambdas,
  adaptiveLassoWeights,
  sparseParameters = NULL,
  eta = 2,
  sig = 10^(-5),
  initialStepsize = 1,
  stepsizeMin = 1/(10^30),
  stepsizeMax = 10^30,
  GISTLinesearchCriterion = "monotone",
  GISTNonMonotoneNBack = 5,
  maxIter_out = 100,
  maxIter_in = 1000,
  break_outer = c(parameterChange = 10^(-5)),
  scaleLambdaWithN = TRUE,
  sampleSize,
  approxFirst = F,
  numStart = 3,
  controlApproxOptimizer,
  verbose = 0
)
}
\arguments{
\item{cpptsemObject}{Fitted object of class cpptsem}

\item{dataset}{only required if objective = "Kalman" and ctsemObject is of type ctsemInit. Please provide a data set in wide format compatible to ctsemOMX}

\item{objective}{which objective should be used? Possible are "ML" (Maximum Likelihood) or "Kalman" (Kalman Filter)}

\item{regIndicators}{Vector with names of regularized parameters}

\item{targetVector}{named vector with values towards which the parameters are regularized}

\item{lambdas}{Vector with lambda values that should be tried}

\item{adaptiveLassoWeights}{weights for the adaptive lasso.}

\item{sparseParameters}{labeled vector with parameter estimates of the most sparse model.}

\item{eta}{if the current step size fails, eta will decrease the step size. Must be > 1}

\item{sig}{Controls the sigma parameter in Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). An improved GLMNET for l1-regularized logistic regression. The Journal of Machine Learning Research, 13, 1999â€“2030. https://doi.org/10.1145/2020408.2020421, Equation 20. Defaults to 0. Has to be in 0 < sigma < 1}

\item{initialStepsize}{intial step size tested at the beginning of each optimization iteration}

\item{stepsizeMin}{Minimal acceptable step size. Must be > 0. A larger number corresponds to a smaller step from one to the next iteration. All step sizes will be computed as described by Gong et al. (2013)}

\item{stepsizeMax}{Maximal acceptable step size. Must be > stepsizeMin. A larger number corresponds to a smaller step from one to the next iteration. All step sizes will be computed as described by Gong et al. (2013)}

\item{GISTLinesearchCriterion}{criterion for accepting a step. Possible are 'monotone' which enforces a monotone decrease in the objective function or 'non-monotone' which also accepts some increase.}

\item{GISTNonMonotoneNBack}{in case of non-monotone line search: Number of preceding regM2LL values to consider}

\item{maxIter_out}{maximal number of outer iterations}

\item{maxIter_in}{maximal number of inner iterations}

\item{break_outer}{Stopping criterion for outer iterations. It has to be a named value. By default (name: gradient), a relative first-order condition is checked, where the maximum absolute value of the gradients is compared to break_outer (see https://de.mathworks.com/help/optim/ug/first-order-optimality-measure.html). Alternatively, an absolute tolerance can be passed to the function (e.g., break_outer = c("gradient" = .0001)). Instead of relative gradients, the change in parameters can used as breaking criterion. To this end, use c("parameterChange" = .00001)}

\item{scaleLambdaWithN}{Boolean: Should the penalty value be scaled with the sample size? True is recommended, as the likelihood is also sample size dependent}

\item{sampleSize}{sample size for scaling lambda with N}

\item{approxFirst}{Should approximate optimization be used first to obtain start values for exact optimization?}

\item{numStart}{Used if approxFirst = 3. regCtsem will try numStart+2 starting values (+2 because it will always try the current best and the parameters provided in sparseParameters)}

\item{controlApproxOptimizer}{settings passed to optimx or Rsolnp}

\item{verbose}{0 (default), 1 for convergence plot, 2 for parameter convergence plot and line search progress. Set verbose = -1 to use a C++ implementation of GIST (not much faster which is why the easier to handle R implementation is the default)}
}
\description{
General Iterative Shrinkage and Thresholding Algorithm based on Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). A General Iterative Shrinkage and Thresholding Algorithm for Non-convex Regularized Optimization Problems. In S. Dasgupta & D. McAllester (Eds.), Proceedings of Machine Learning Research (PMLR; Vol. 28, Issue 2, pp. 37--45). PMLR. http://proceedings.mlr.press
}
\details{
GIST minimizes a function of form f(theta) = l(theta) + g(theta), where l is the likelihood and g is a penalty function. Various penalties are supported, however currently only lasso and adaptive lasso are implemented.

NOTE: Function located in file GIST.R
}
