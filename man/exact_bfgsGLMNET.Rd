% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/GLMNET.R
\name{exact_bfgsGLMNET}
\alias{exact_bfgsGLMNET}
\title{exact_bfgsGLMNET}
\usage{
exact_bfgsGLMNET(
  ctsemObject,
  mxObject,
  dataset,
  objective,
  regIndicators,
  lambdas,
  adaptiveLassoWeights,
  sparseParameters = NULL,
  tryCpptsem,
  forceCpptsem = FALSE,
  stepSize = 1,
  lineSearch = "none",
  c1 = 1e-04,
  c2 = 0.9,
  sig = 10^(-5),
  gam = 0,
  differenceApprox = "central",
  initialHessianApproximation = "OpenMx",
  maxIter_out = 100,
  maxIter_in = 1000,
  maxIter_line = 500,
  eps_out = 1e-10,
  eps_in = 1e-10,
  eps_WW = 1e-04,
  scaleLambdaWithN = TRUE,
  sampleSize,
  approxFirst = T,
  numStart = 0,
  approxMaxIt = 5,
  extraTries = 3,
  verbose = 0,
  progressBar = TRUE,
  parallelProgressBar = NULL
)
}
\arguments{
\item{ctsemObject}{Fitted ctsem object}

\item{mxObject}{Object of type MxModel}

\item{dataset}{only required if objective = "Kalman". Please provide a data set in wide format compatible to ctsemOMX}

\item{objective}{which objective should be used? Possible are "ML" (Maximum Likelihood) or "Kalman" (Kalman Filter)}

\item{regIndicators}{Vector with names of regularized parameters}

\item{lambdas}{Vector with lambda values that should be tried}

\item{adaptiveLassoWeights}{weights for the adaptive lasso.}

\item{tryCpptsem}{should regCtsem try to translate the model to cpptsem? This can speed up the computation considerably but might fail for some models}

\item{forceCpptsem}{should cpptsem be enforced even if results differ from ctsem? Sometimes differences between cpptsem and ctsem can result from problems with numerical precision which will lead to the matrix exponential of RcppArmadillo differing from the OpenMx matrix exponential. If you want to ensure the faster optimization, set to TRUE. See vignette("MatrixExponential", package = "regCtsem") for more details}

\item{stepSize}{Initial stepSize of the outer iteration (theta_{k+1} = theta_k + stepSize * Stepdirection)}

\item{lineSearch}{String indicating which linesearch should be used. Defaults to the one described in Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). An improved GLMNET for l1-regularized logistic regression. The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421. Alternatively (not recommended) Wolfe conditions (lineSearch = "Wolfe") can be used in the outer iteration. Setting to "none" is also not recommended!.}

\item{c1}{c1 constant for lineSearch. This constant controls the Armijo condition in lineSearch if lineSearch = "Wolfe"}

\item{c2}{c2 constant for lineSearch. This constant controls the Curvature condition in lineSearch if lineSearch = "Wolfe"}

\item{sig}{only relevant when lineSearch = 'GLMNET'. Controls the sigma parameter in Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). An improved GLMNET for l1-regularized logistic regression. The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421.}

\item{gam}{Controls the gamma parameter in Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). An improved GLMNET for l1-regularized logistic regression. The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421. Defaults to 0.}

\item{differenceApprox}{Which approximation should be used for calculating the gradients in the gradientModel. central is recommended}

\item{initialHessianApproximation}{Which initial hessian approximation should be used? Possible are: 'ident' for an identity matrix and 'OpenMx' (here the hessian approxmiation from the mxObject is used). If the Hessian from 'OpenMx' is not positive definite, the negative Eigenvalues will be 'flipped' to positive Eigenvalues. This works sometimes, but not always. Alternatively, a matrix can be provided which will be used as initial Hessian}

\item{maxIter_out}{Maximal number of outer iterations}

\item{maxIter_in}{Maximal number of inner iterations}

\item{maxIter_line}{Maximal number of iterations for the lineSearch procedure}

\item{eps_out}{Stopping criterion for outer iterations}

\item{eps_in}{Stopping criterion for inner iterations}

\item{eps_WW}{Stopping criterion for weak Wolfe line search. If the upper - lower bound of the interval is < epsWW, line search will be stopped and stepSize will be returned}

\item{scaleLambdaWithN}{Boolean: Should the penalty value be scaled with the sample size? True is recommended, as the likelihood is also sample size dependent}

\item{sampleSize}{sample size for scaling lambda with N}

\item{approxFirst}{Should approximate optimization be used first to obtain start values for exact optimization?}

\item{numStart}{Used if approxFirst = 3. regCtsem will try numStart+2 starting values (+2 because it will always try the current best and the parameters provided in sparseParameters)}

\item{approxMaxIt}{Used if approxFirst. How many outer iterations should be given to each starting values vector?}

\item{extraTries}{number of extra tries in mxTryHard for warm start}

\item{verbose}{0 (default), 1 for convergence plot, 2 for parameter convergence plot and line search progress}
}
\description{
Performs GLMNET (see Friedman, 2010 & Yuan, 2011) with bfgs approximated Hessian
}
\details{
NOTE: Function located in file GLMNET.R
}
\examples{
library(regCtsem)
library(ctsemOMX)

# The following example is taken directly from the examples provided in the ctFit documentation of ctsemOMX
### Example from Voelkle, Oud, Davidov, and Schmidt (2012) - anomia and authoritarianism.
data(AnomAuth)
AnomAuthmodel <- ctModel(LAMBDA = matrix(c(1, 0, 0, 1), nrow = 2, ncol = 2),
                         Tpoints = 5, n.latent = 2, n.manifest = 2, MANIFESTVAR=diag(0, 2), TRAITVAR = NULL)
AnomAuthfit <- ctFit(AnomAuth, AnomAuthmodel, fit = T)

# with GLMNET
## with standardization
reg <- exact_bfgsGLMNET(mxObject = AnomAuthfit$mxobj, regIndicators = c("drift_eta2_eta1", "drift_eta1_eta2"), lambdas = rev(seq(0,1,.1)), standardizeDrift = TRUE)
reg$regM2LL
reg$thetas

## without standardization
reg2 <- exact_bfgsGLMNET(mxObject = AnomAuthfit$mxobj, regIndicators = c("drift_eta2_eta1", "drift_eta1_eta2"), lambdas = seq(0,1,.1), standardizeDrift = FALSE)
reg2$regM2LL
reg2$thetas
}
